<!DOCTYPE html> <html> <head> <meta http-equiv="Content-Type" content="text/html; charset=UTF-8"> <meta charset="utf-8"> <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"> <meta http-equiv="X-UA-Compatible" content="IE=edge"> <title> How to Think About TPUs | How To Scale Your Model </title> <meta name="author" content=" "> <meta name="description" content="이 섹션에서는 TPU가 어떻게 작동하는지, 멀티칩 훈련 및 추론을 위해 어떻게 서로 연결되는지, 그리고 이가 우리가 즐겨 사용하는 알고리즘의 성능에 어떤 영향을 미치는지에 대해 자세히 다룹니다. GPU 사용자에게도 유용한 정보가 있습니다!"> <meta name="keywords" content="scaling, jax, llms, transformers, tpus, google, deepmind, parallelism, pallas"> <link rel="stylesheet" href="/scaling-book/assets/css/bootstrap.min.css?a4b3f509e79c54a512b890d73235ef04"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/css/mdb.min.css" integrity="sha256-jpjYvU3G3N6nrrBwXJoVEYI/0zw8htfFnhT9ljN3JJw=" crossorigin="anonymous"> <link defer rel="stylesheet" href="/scaling-book/assets/css/academicons.min.css?f0b7046b84e425c55f3463ac249818f5"> <link defer rel="stylesheet" href="/scaling-book/assets/css/scholar-icons.css?62b2ac103a88034e6882a5be5f3e2772"> <link defer rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons&amp;display=swap"> <link defer rel="stylesheet" href="/scaling-book/assets/css/jekyll-pygments-themes-vs.css?4ee1a2facd1a8a76347f4bd43a740500" media="" id="highlight_theme_light"> <link rel="shortcut icon" href="/scaling-book/assets/img/favicon.png?fddbd8c2ec231ba2060e67c85de32a55"> <link rel="stylesheet" href="/scaling-book/assets/css/main.css?d41d8cd98f00b204e9800998ecf8427e"> <link rel="canonical" href="https://jh-michael-shin.github.io/scaling-book/tpus/"> <script src="/scaling-book/assets/js/theme.js?9a0c749ec5240d9cda97bc72359a72c0"></script> <link defer rel="stylesheet" href="/scaling-book/assets/css/jekyll-pygments-themes-native.css?5847e5ed4a4568527aa6cfab446049ca" media="none" id="highlight_theme_dark"> <script>
    initTheme();
  </script> <script src="/scaling-book/assets/js/distillpub/template.v2.js"></script> <script src="/scaling-book/assets/js/distillpub/transforms.v2.js"></script> <style type="text/css">{{page._styles}}</style> </head> <body> <d-front-matter> <script async type="text/json">
      {
            "title": "How to Think About TPUs",
            "description": "이 섹션에서는 TPU가 어떻게 작동하는지, 멀티칩 훈련 및 추론을 위해 어떻게 서로 연결되는지, 그리고 이가 우리가 즐겨 사용하는 알고리즘의 성능에 어떤 영향을 미치는지에 대해 자세히 다룹니다. GPU 사용자에게도 유용한 정보가 있습니다!",
            "published": "February 04, 2025",
            "authors": [
              
              {
                "author": "Jacob Austin",
                "authorURL": "https://www.jacobaustin.org/",
                "affiliations": [
                  {
                    "name": "Google DeepMind",
                    "url": ""
                  }
                ]
              },
              
              {
                "author": "Sholto Douglas",
                "authorURL": "https://x.com/_sholtodouglas",
                "affiliations": [
                  {
                    "name": "",
                    "url": ""
                  }
                ]
              },
              
              {
                "author": "Roy Frostig",
                "authorURL": "https://cs.stanford.edu/~rfrostig/",
                "affiliations": [
                  {
                    "name": "",
                    "url": ""
                  }
                ]
              },
              
              {
                "author": "Anselm Levskaya",
                "authorURL": "https://anselmlevskaya.com/",
                "affiliations": [
                  {
                    "name": "",
                    "url": ""
                  }
                ]
              },
              
              {
                "author": "Charlie Chen",
                "authorURL": "https://x.com/charliexychen",
                "affiliations": [
                  {
                    "name": "",
                    "url": ""
                  }
                ]
              },
              
              {
                "author": "Sharad Vikram",
                "authorURL": "https://sharadvikram.com/",
                "affiliations": [
                  {
                    "name": "",
                    "url": ""
                  }
                ]
              },
              
              {
                "author": "Federico Lebron",
                "authorURL": "https://fedelebron.com/",
                "affiliations": [
                  {
                    "name": "",
                    "url": ""
                  }
                ]
              },
              
              {
                "author": "Peter Choy",
                "authorURL": "https://x.com/pchoy95",
                "affiliations": [
                  {
                    "name": "",
                    "url": ""
                  }
                ]
              },
              
              {
                "author": "Vinay Ramasesh",
                "authorURL": "https://x.com/vinayramasesh",
                "affiliations": [
                  {
                    "name": "",
                    "url": ""
                  }
                ]
              },
              
              {
                "author": "Albert Webson",
                "authorURL": "https://representation.ai/",
                "affiliations": [
                  {
                    "name": "",
                    "url": ""
                  }
                ]
              },
              
              {
                "author": "Reiner Pope<sup>*</sup>",
                "authorURL": "https://x.com/reinerpope",
                "affiliations": [
                  {
                    "name": "",
                    "url": ""
                  }
                ]
              }
              
            ],
            "katex": {
              "delimiters": [
                {
                  "left": "$",
                  "right": "$",
                  "display": false
                },
                {
                  "left": "$$",
                  "right": "$$",
                  "display": true
                }
              ]
            }
          }
    </script> </d-front-matter> <header> <script>
    function goToTop() {
      document.body.scrollTop = 0; // For Safari
      document.documentElement.scrollTop = 0; // For Chrome, Firefox, IE and Opera
    }

    // When the user scrolls down 20px from the top of the document, show the button
    window.onscroll = function() {scrollFunction()};

    function scrollFunction() {
      // Get the button:
      let mybutton = document.getElementById("top-button");

      if (document.body.scrollTop > 40 || document.documentElement.scrollTop > 40) {
        mybutton.style.display = "block";
      } else {
        mybutton.style.display = "none";
      }
  }
  </script> <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top" role="navigation"> <div class="container"> <a class="navbar-brand title font-weight-lighter" href="/scaling-book"> How To Scale Your Model </a> <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation"> <span class="sr-only">Toggle navigation</span> <span class="icon-bar top-bar"></span> <span class="icon-bar middle-bar"></span> <span class="icon-bar bottom-bar"></span> </button> <div class="left-button section-button"><a href="../roofline"><svg viewbox="-78.5 0 512 512"><path d="M257 64L291 98 128 262 291 426 257 460 61 262 257 64Z"></path></svg></a></div> <div class="right-button section-button"><a href="../sharding"><svg viewbox="-78.5 0 512 512"><path d="M98 460L64 426 227 262 64 98 98 64 294 262 98 460Z"></path></svg></a></div> <div class="collapse navbar-collapse text-right" id="navbarNav"> <ul class="navbar-nav ml-auto flex-nowrap"> <li class="nav-item "> <a class="nav-link" href="/scaling-book/"> </a> </li> <li class="nav-item nav-hidden"><a class="nav-link" onclick="goToTop()" id="top-button" style="display: none;">Back to Top</a></li> <li class="nav-item nav-hidden"><p class="nav-link"></p></li> <li class="nav-item nav-hidden"><a class="nav-link" href="../roofline">Previous Part</a></li> <li class="nav-item nav-hidden"><a class="nav-link" href="../sharding">Next Part</a></li> <li class="nav-item nav-hidden"><p class="nav-link"></p></li> <li class="nav-item dropdown "> <a class="nav-link dropdown-toggle" href="#" id="navbarDropdown" role="button" data-toggle="dropdown" aria-haspopup="true" aria-expanded="false">Sections </a> <div class="dropdown-menu dropdown-menu-right" aria-labelledby="navbarDropdown"> <a class="dropdown-item " href="/scaling-book/index">Part 0. Introduction</a> <a class="dropdown-item " href="/scaling-book/roofline">Part 1. Intro to Rooflines</a> <a class="dropdown-item " href="/scaling-book/tpus">Part 2. All About TPUs</a> <a class="dropdown-item " href="/scaling-book/sharding">Part 3. Sharded Matmuls</a> <a class="dropdown-item " href="/scaling-book/transformers">Part 4. Transformers</a> <a class="dropdown-item " href="/scaling-book/training">Part 5. Training</a> <a class="dropdown-item " href="/scaling-book/applied-training">Part 6. Training LLaMA</a> <a class="dropdown-item " href="/scaling-book/inference">Part 7. Inference</a> <a class="dropdown-item " href="/scaling-book/applied-inference">Part 8. Serving LLaMA</a> <a class="dropdown-item " href="/scaling-book/profiling">Part 9. Profiling</a> <a class="dropdown-item " href="/scaling-book/jax-stuff">Part 10. All About JAX</a> <a class="dropdown-item " href="/scaling-book/conclusion">Part 11. Conclusions</a> </div> </li> <li class="toggle-container"> <button id="light-toggle" title="Change theme"> <i class="ti ti-sun-moon" id="light-toggle-system"></i> <i class="ti ti-moon-filled" id="light-toggle-dark"></i> <i class="ti ti-sun-filled" id="light-toggle-light"></i> </button> </li> </ul> </div> </div> </nav> <progress id="progress" value="0"> <div class="progress-container"> <span class="progress-bar"></span> </div> </progress> </header> <div class="post distill"> <d-title> <h1>How to Think About TPUs</h1> <p>Part 2 of <a href="/scaling-book">How To Scale Your Model</a> (<a href="../roofline">Part 1: Rooflines</a> | <a href="../sharding">Part 3: Sharding</a>)</p> <p>이 섹션에서는 TPU가 어떻게 작동하는지, 멀티칩 훈련 및 추론을 위해 어떻게 서로 연결되는지, 그리고 이가 우리가 즐겨 사용하는 알고리즘의 성능에 어떤 영향을 미치는지에 대해 자세히 다룹니다. GPU 사용자에게도 유용한 정보가 있습니다!</p> </d-title> <d-byline></d-byline> <d-article> <d-contents> <nav class="l-text figcaption"> <h3>Contents</h3> <div> <a href="#what-is-a-tpu">What Is a TPU?</a> </div> <div> <a href="#tpu-networking">TPU Networking</a> </div> <div> <a href="#key-takeaways">Key Takeaways</a> </div> <div> <a href="#worked-problems">Worked Problems</a> </div> <div> <a href="#appendix">Appendix</a> </div> <div> <a href="#"></a> </div> <ul> <li> <a href="#appendix-a-all-about-gpus">Appendix A: All about GPUs</a> </li> <li> <a href="#appendix-b-how-does-a-systolic-array-work">Appendix B: How does a systolic array work?</a> </li> <li> <a href="#appendix-c-more-on-tpu-internals">Appendix C: More on TPU internals</a> </li> </ul> </nav> </d-contents> <p class="takeaway"> <b>번역 안내:</b> 원저자(<a href="https://www.jacobaustin.org/" rel="external nofollow noopener" target="_blank">Jacob Austin</a>)의 허락을 받아 원문을 번역 중입니다.<br> 해당 글의 1인칭은 원문 저자를 지칭합니다.<br> 원문: <a href="https://jax-ml.github.io/scaling-book/" rel="external nofollow noopener" target="_blank">How to Scale Your Model</a><br> 번역: <a href="https://www.linkedin.com/in/michael-shin-3522a6189/" rel="external nofollow noopener" target="_blank">신종훈</a></p> <h2 id="what-is-a-tpu">What Is a TPU?</h2> <p><strong>TPU는 기본적으로 행렬 곱셈에 특화된 연산 코어(TensorCore)가 빠른 메모리 스택(고대역폭 메모리 또는 HBM)에 부착된 형태입니다<d-cite key="tpu_paper"></d-cite>.</strong> 아래는 TPU의 다이어그램입니다:</p> <figure> <picture> <source class="responsive-img-srcset" srcset="/scaling-book/assets/img/tpu-chip-480.webp 480w,/scaling-book/assets/img/tpu-chip-800.webp 800w,/scaling-book/assets/img/tpu-chip-1400.webp 1400w," type="image/webp" sizes="95vw"></source> <img src="/scaling-book/assets/img/tpu-chip.png" class="img-fluid" width="100%" height="auto" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> <figcaption class="caption"><b>Figure:</b> TPU 칩의 기본 구성 요소. TensorCore는 회색의 왼쪽 박스로, 행렬 곱셈 유닛(MXU), 벡터 유닛(VPU), 벡터 메모리(VMEM)를 포함합니다.</figcaption> </figure> <p>TensorCore는 기본적으로 정말 뛰어난 행렬 곱셈 기계라고 생각할 수 있지만, 다른 주목할 만한 몇 가지 기능도 있습니다. TensorCore에는 세 가지 핵심 유닛이 있습니다:</p> <ul> <li> <p><strong>MXU</strong> (Matrix Multiply Unit)는 TensorCore의 핵심입니다. 대부분의 TPU 세대에서, MXU는 시스톨릭 배열(systolic array)을 사용하여 8 사이클마다 <code class="language-plaintext highlighter-rouge">bfloat16[8,128] @ bf16[128,128] -&gt; f32[8,128]</code> 행렬 곱셈을 한 번 수행합니다</p> </li> <li> <strong>MXU</strong> (Matrix Multiply Unit)는 TensorCore의 핵심입니다. 대부분의 TPU 세대에서, MXU는 시스톨릭 배열(systolic array)을 사용하여 8 사이클마다 <code class="language-plaintext highlighter-rouge">bfloat16[8,128] @ bf16[128,128] -&gt; f32[8,128]</code> 행렬 곱셈<d-footnote>TPU v6e (Trillium)는 256x256 MXU를 사용하며, 이전 세대는 모두 128x128을 사용합니다.</d-footnote> 한 번 수행합니다 (<a href="#appendix-b-how-does-a-systolic-array-work">Appendix B</a> 참조). <ul> <li>이는 TPU v5e에서 1.5GHz로 작동할 때 MXU당 약 <code class="language-plaintext highlighter-rouge">5e13</code> bf16 FLOPs/s에 해당합니다. 대부분의 TensorCore에는 2개 또는 4개의 MXU가 있으므로, 예를 들어 TPU v5e의 총 bf16 FLOPs/s는 <code class="language-plaintext highlighter-rouge">2e14</code>입니다.</li> <li>TPU는 또한 더 높은 처리량을 가진 더 낮은 정밀도의 matmul도 지원합니다 (예: 각 TPU v5e 칩은 <code class="language-plaintext highlighter-rouge">4e14</code> int8 OPs/s를 수행할 수 있습니다).</li> </ul> </li> <li> <strong>VPU</strong> (Vector Processing Unit)는 ReLU 활성화나 벡터 간의 원소별 덧셈 또는 곱셈과 같은 일반적인 수학 연산을 수행합니다. Reduction(sums) 연산도 여기서 수행됩니다. 자세한 내용은 <a href="#appendix-c-tpu-internals">Appendix C</a> 를 참조하시면 됩니다.</li> <li> <strong>VMEM</strong> (Vector Memory)은 TensorCore 내부에 위치한 온칩(on-chip) 스크래치패드로, 연산 유닛에 근접해있습니다. HBM보다 훨씬 작지만(예: TPU v5e에서는 128MiB) MXU와의 대역폭은 훨씬 높습니다. VMEM은 CPU의 L1/L2 캐시와 꽤나 유사하게 작동하지만, 훨씬 크고 프로그래머가 제어(programmer-controlled)할 수 있습니다. HBM의 데이터는 TensorCore가 계산을 수행하기 전에 VMEM으로 복사되어야 합니다.</li> </ul> <p><strong>TPU는 행렬 곱셈이 아주, 아주 빠릅니다</strong>. TPU의 주요 업무이기도 하고 잘 하기도 합니다. 지금까지 가장 강력한 TPU 중 하나인 <a href="https://cloud.google.com/tpu/docs/v5p#system_architecture" rel="external nofollow noopener" target="_blank">TPU v5p</a>는 코어당 초당 <code class="language-plaintext highlighter-rouge">2.5e14</code> bf16 FLOPs / second 또는 칩당 <code class="language-plaintext highlighter-rouge">5e14</code> bf16 FLOPs / second 을 수행할 수 있습니다. 8960개 칩으로 구성된 단일 pod는 초당 4 exaflops를 처리할 수 있습니다. 이는 <em>어마어마한</em> 양입니다. 이는 세계에서 가장 강력한 슈퍼컴퓨터 중 하나이며, 구글은 이를 다수 보유하고 있습니다.<d-footnote>TPU와 특히 이의 시스톨릭 배열(systolic arrays)이 이토록 강력한 하드웨어 가속기인 이유는, 행렬 곱셈이 $O(n^2)$ 바이트에 대해 $O(n^3)$의 연산을 사용하는 몇 안 되는 알고리즘 중 하나이기 때문입니다. 이로 인해 일반적인 ALU가 메모리 대역폭이 아닌 연산 자체에 의해 병목 현상을 겪기 매우 쉽습니다.</d-footnote></p> <p>위의 다이어그램에는 제어 흐름 처리(control flow handling)에 사용되는 SMEM 및 스칼라(scalar) 유닛과 같은 몇 가지 다른 구성 요소도 포함되어 있으며, 이는 <a href="#appendix-c-tpu-internals">Appendix C</a>에서 짧게 다루지만, 꼭 이해하셔야 하지는 않습니다. 반면에 HBM은 중요하면서 또한 비교적 간단합니다:</p> <ul> <li> <p><strong>HBM</strong> (High Bandwidth Memory) 은 TensorCore에서 사용할 텐서를 저장하는 큰 용량의 빠른 메모리입니다. HBM은 보통 수십 기가바이트의 용량을 가집니다(예를 들자면, <a href="https://cloud.google.com/tpu/docs/v5e#system_architecture" rel="external nofollow noopener" target="_blank">TPU v5e는 16GiB의 HBM을 가짐</a>).</p> <ul> <li> <p>계산이 필요할 때, 텐서는 HBM에서 VMEM(아래 예제 있음)을 통해 스트리밍되어 MXU로 들어가고, 결과는 VMEM에서 다시 HBM으로 쓰입니다.</p> </li> <li> <p>HBM과 TensorCore(VMEM을 통해) 간의 대역폭은 “HBM 대역폭” (보통 1-2TB/sec)이라 하며, 메모리 병목(memory-bound) 워크로드에서 계산이 얼마나 빨리 수행할 수 있는지의 제약 사항이 됩니다.</p> </li> </ul> </li> </ul> <p><strong>보통 모든 TPU 연산은 파이프라인화되고 중첩됩니다.</strong> matmul $X \cdot A \to Y$ 를 수행하기 위해, TPU는 먼저 HBM에서 $A$ 와 $X$ 행렬의 청크를 VMEM으로 복사한 다음, 이를 MXU로 로드하여 8x128($X$의 경우) 및 128x128($A$의 경우) 청크를 곱하고, 그 결과를 청크 단위로 다시 HBM에 복사합니다. 이를 효율적으로 수행하기 위해, matmul은 VMEM으로/에서 복사하는 작업이 MXU 작업과 중첩되도록 파이프라인화됩니다. 이를 통해 MXU는 메모리 전송을 기다리지 않고 계속 작동할 수 있으며, matmul이 메모리 병목이 아닌 연산 병목 상태를 유지하게 합니다.</p> <p>다음은 HBM에서 원소별 곱셈(elementwise product)을 수행하는 방법의 예제입니다:</p> <figure> <picture> <source class="responsive-img-srcset" srcset="/scaling-book/assets/img/pointwise-product-480.webp 480w,/scaling-book/assets/img/pointwise-product-800.webp 800w,/scaling-book/assets/img/pointwise-product-1400.webp 1400w," type="image/webp" sizes="95vw"></source> <img src="/scaling-book/assets/img/pointwise-product.gif" width="100%" height="auto" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> <figcaption class="caption"><b>Figure:</b> HBM에서 바이트를 로드하여 TPU에서 원소별 곱셈(pointwise product)을 수행하는 애니메이션. 바이트가 메모리에서 청크 단위로 스트리밍되고, 전체 배열이 구체화되기를 기다리지 않고 부분 결과가 파이프라인으로 다시 전송되는 방식을 주의 깊게 봐주세요.</figcaption> </figure> <p>matmul은 VPU/Vector Unit 대신 MXU로 로드되고, 동일한 가중치 청크가 여러 활성화 청크에 사용되므로 로드 및 저장 순서가 다르다는 점을 제외하면 거의 동일하게 보일 것입니다. 데이터 청크가 VMEM으로, 다음 VREG(vector registers)로, 다음 Vector Unit으로, 그리고 다시 VMEM과 HBM으로 스트리밍되는 것을 볼 수 있습니다. 곧 보게 되겠지만, HBM에서 VMEM으로의 로드가 Vector Unit(또는 MXU)의 FLOPs보다 느리면, VPU나 MXU에 작업이 공급되지 않아 “대역폭 병목” 상태가 됩니다.</p> <p class="takeaway"><strong>Key takeaway:</strong> TPU는 아주 심플합니다. HBM에서 VMEM으로 가중치를 로드한 다음, VMEM에서 초당 약 200조 번의 multiply-adds 연산을 수행할 수 있는 시스톨릭 배열로 로드합니다. HBM $\leftrightarrow$ VMEM 그리고 VMEM $\leftrightarrow$ 시스톨릭 배열 대역폭은 TPU가 효율적으로 수행할 수 있는 계산에 대한 근본적인 한계를 설정합니다.</p> <p><strong>VMEM과 arithmetic intensity:</strong> VMEM은 HBM보다 훨씬 작지만 MXU로의 대역폭은 훨씬 높습니다. <a href="../roofline">섹션 1</a>에서 보았듯이, 이는 알고리즘의 모든 입력/출력을 VMEM에 맞출 수 있다면 통신 병목에 부딪힐 가능성이 훨씬 작아진다는 것을 의미합니다. 이는 계산의 arithmetic intensity가 낮을 때 특히 유용합니다:</p> <p>VMEM 대역폭은 HBM 대역폭보다 약 22배 높으므로, VMEM에서 읽고 쓰는 MXU 연산은 최대 FLOPs 활용도를 달성하기 위해 10-20의 arithmetic intensity만 필요합니다. 즉, 가중치를 HBM 대신 VMEM에 맞출 수 있다면, 훨씬 작은 배치 크기에서도 행렬 곱셈이 FLOPs 병목 상태가 될 수 있습니다. 그리고 근본적으로 낮은 arithmetic intensity를 가진 알고리즘도 여전히 효율적일 수 있다는 의미입니다. 다만 VMEM이 너무 작아서 이것이 종종 어려운 과제가 됩니다.<d-footnote>우리는 때때로 VMEM 프리페칭(prefetching)에 대해 이야기하는데, 이는 matmul에 대한 로딩 비용을 가리기 위해 VMEM에 가중치를 미리 로드하는 것을 의미합니다. 예를 들어, 일반적인 Transformer에서 어텐션 중에 큰 피드포워드 가중치를 VMEM으로 로드하여, 메모리 대역폭 병목 상태일 경우 가중치 로드 비용을 숨길 수 있습니다. 이를 위해서는 가중치가 충분히 작거나, 단일 레이어를 VMEM에 넣고도 공간이 남을 만큼 충분히 샤딩되어야 합니다.</d-footnote></p> <figure> <picture> <source class="responsive-img-srcset" srcset="/scaling-book/assets/img/tpu-bandwidth-480.webp 480w,/scaling-book/assets/img/tpu-bandwidth-800.webp 800w,/scaling-book/assets/img/tpu-bandwidth-1400.webp 1400w," type="image/webp" sizes="95vw"></source> <img src="/scaling-book/assets/img/tpu-bandwidth.png" class="img-fluid" width="100%" height="auto" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> <p><strong>A TPU chip typically (but not always) consists of two TPU cores which share memory and can be thought of as one large accelerator</strong> with twice the FLOPs (known as a “megacore” configuration). This has been true since TPU v4. Older TPU chips have separate memory and are regarded as two separate accelerators (TPU v3 and older). Inference-optimized chips like the TPU v5e only have one TPU core per chip.</p> <figure> <picture> <source class="responsive-img-srcset" srcset="/scaling-book/assets/img/cores-480.webp 480w,/scaling-book/assets/img/cores-800.webp 800w,/scaling-book/assets/img/cores-1400.webp 1400w," type="image/webp" sizes="95vw"></source> <img src="/scaling-book/assets/img/cores.png" class="img-fluid img-small" width="100%" height="auto" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> <p><strong>Chips</strong> are arranged in <strong>sets of 4 on a ‘tray’</strong> connected to a <strong>CPU host via PCIe network.</strong> This is the format most readers will be familiar with, 4 chips (8 cores, though usually treated as 4 logical megacores) exposed through Colab or a single TPU-VM. For inference chips like the TPU v5e, we have 2 trays per host, instead of 1, but also only 1 core per chip, giving us 8 chips = 8 cores.<d-footnote>On Cloud TPU VMs, each tray is exposed as part of a separate VM, so there are once again 4 cores visible.</d-footnote></p> <figure> <picture> <source class="responsive-img-srcset" srcset="/scaling-book/assets/img/pcie-480.webp 480w,/scaling-book/assets/img/pcie-800.webp 800w,/scaling-book/assets/img/pcie-1400.webp 1400w," type="image/webp" sizes="95vw"></source> <img src="/scaling-book/assets/img/pcie.png" class="img-fluid" width="100%" height="auto" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> <p><strong>PCIe bandwidth is limited:</strong> Like the HBM $\leftrightarrow$ VMEM link, the CPU $\leftrightarrow$ HBM PCIe connection has a specific bandwidth that limits how quickly you can load from host memory to HBM or vice-versa. PCIe bandwidth for TPU v4 is 16GB / second each way, for example, so close to 100x slower than HBM. We <em>can</em> load/offload data into the host (CPU) RAM, but not very quickly.</p> <h2 id="tpu-networking">TPU Networking</h2> <p><strong>Chips are connected to each other through the ICI network in a Pod</strong>. In older generations (TPU v2 and TPU v3), inference chips (e.g., TPU v5e), and Trilium (TPU v6e), ICI (“inter-chip interconnects”) connects the 4 nearest neighbors (with edge links to form a 2D torus). TPU v4 and TPU v5p are connected to the nearest 6 neighbors (forming a 3D torus). Note these connections do <strong>not</strong> go through their hosts, they are direct links between chips.</p> <figure> <picture> <source class="responsive-img-srcset" srcset="/scaling-book/assets/img/ici-wraparound-480.webp 480w,/scaling-book/assets/img/ici-wraparound-800.webp 800w,/scaling-book/assets/img/ici-wraparound-1400.webp 1400w," type="image/webp" sizes="95vw"></source> <img src="/scaling-book/assets/img/ici-wraparound.png" class="img-fluid img-small" width="100%" height="auto" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> <p>The toroidal structure reduces the maximum distance between any two nodes from $N$ to $N / 2$, making communication much faster. TPUs also have a “twisted torus” configuration that wraps the torus in a Mobius-strip like topology to further reduce the average distance between nodes.</p> <p><strong>TPU pods (connected by ICI) can get really big:</strong> the maximum pod size (called a <strong>superpod</strong>) is <code class="language-plaintext highlighter-rouge">16x16x16</code> for TPU v4 and <code class="language-plaintext highlighter-rouge">16x20x28</code> for TPU v5p. These large pods are composed of reconfigurable cubes of <code class="language-plaintext highlighter-rouge">4x4x4</code> chips connected by <a href="https://arxiv.org/pdf/2208.10041" rel="external nofollow noopener" target="_blank">optical wraparound links</a><d-footnote>The optical switch is simply a reconfigurable connection with the same ICI bandwidth. It just lets us connect cubes while retaining a wraparound link.</d-footnote> that we can reconfigure to connect very large topologies.</p> <figure> <picture> <source class="responsive-img-srcset" srcset="/scaling-book/assets/img/tpu-rack-480.webp 480w,/scaling-book/assets/img/tpu-rack-800.webp 800w,/scaling-book/assets/img/tpu-rack-1400.webp 1400w," type="image/webp" sizes="95vw"></source> <img src="/scaling-book/assets/img/tpu-rack.png" class="img-fluid" width="100%" height="auto" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> <p>Smaller topologies (e.g. <code class="language-plaintext highlighter-rouge">2x2x1</code>, <code class="language-plaintext highlighter-rouge">2x2x2</code>) can also be requested, albeit with no wraparounds. This is an important caveat, since it typically doubles the time of most communication. Any multiple of a full cube (e.g. <code class="language-plaintext highlighter-rouge">4x4x4</code> or <code class="language-plaintext highlighter-rouge">4x4x8</code>) will have wraparounds provided by the optical switches.<d-footnote>Note that a `2x2x4` won't have any wraparounds since they are provided by the optical switches which are only available on a full cube. A TPU v5e 8x16 _will_ have a wraparound on the longer axis, however, since it doesn't use reconfigurable optical networking.</d-footnote></p> <figure> <picture> <source class="responsive-img-srcset" srcset="/scaling-book/assets/img/subslices-480.webp 480w,/scaling-book/assets/img/subslices-800.webp 800w,/scaling-book/assets/img/subslices-1400.webp 1400w," type="image/webp" sizes="95vw"></source> <img src="/scaling-book/assets/img/subslices.png" class="img-fluid" width="100%" height="auto" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> <p>TPU v5e and Trillium pods consist of a single <code class="language-plaintext highlighter-rouge">16x16</code> 2D torus with wraparounds along any axis of size 16 (meaning an <code class="language-plaintext highlighter-rouge">8x16</code> has a wraparound on the long axis). TPUs v5e and v6e (Trillium) cannot expand beyond a 16x16 torus but pods can still communicate with each other over standard data-center networking (DCN), which connects TPU hosts to each other. Again, smaller topologies can be requested without wraps on dims $&lt;16$.</p> <figure> <picture> <source class="responsive-img-srcset" srcset="/scaling-book/assets/img/more-subslices-480.webp 480w,/scaling-book/assets/img/more-subslices-800.webp 800w,/scaling-book/assets/img/more-subslices-1400.webp 1400w," type="image/webp" sizes="95vw"></source> <img src="/scaling-book/assets/img/more-subslices.png" class="img-fluid" width="100%" height="auto" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> <p><strong>This nearest-neighbor connectivity is a key difference between TPUs and GPUs</strong>. GPUs are connected with a hierarchy of switches that approximate a point-to-point connection between every GPU, rather than using local connections like a TPU. Typically, GPUs within a node (8 GPUs for H100 or as many as 500 for B200) are directly connected, while larger topologies require O(log(N)) hops between each GPU. On the one hand, that means GPUs can send arbitrary data within a node in a single low-latency hop. On the other hand, TPUs are dramatically cheaper (since NVLink switches are expensive) and simpler to wire together, and can scale to much larger topologies because the number of links per device and the bandwidth per device is constant.</p> <p><strong>ICI is very fast relative to DCN, but is still slower than HBM bandwidth.</strong> For instance, a <a href="https://cloud.google.com/tpu/docs/v5p#system_architecture" rel="external nofollow noopener" target="_blank">TPU v5p</a> has:</p> <ul> <li> <code class="language-plaintext highlighter-rouge">2.5e12</code> bytes/s (2.5 TB/s) of HBM bandwidth per chip.</li> <li> <code class="language-plaintext highlighter-rouge">9e10</code> bytes/s (90<d-footnote>The page above lists 100 GB/s of bandwidth, which is slightly different from what's listed here. TPU ICI links have slightly different bandwidths depending on the operation being performed. You can generally use the numbers in this doc without worry.</d-footnote> GB/s) of ICI bandwidth per axis, with 3 axes per chip.</li> <li> <code class="language-plaintext highlighter-rouge">2.5e10</code> bytes/s (25 GB/s) of DCN (egress) bandwidth per host. Since we typically have 8 TPUs per host, this is really closer to <code class="language-plaintext highlighter-rouge">3.1e9</code> bytes / s / chip.</li> </ul> <p>This means that when we split models across multiple chips, we need to be careful to avoid bottlenecking the MXU with slower cross-device communication.</p> <p><strong>Multi-slice training:</strong> A set of ICI-connected TPUs is called a <strong>slice</strong>. Different slices can be connected between each other using DCN, for instance to link slices on different pods. Since DCN is a much slower connection than ICI, one should try to limit how much our computation has to wait for data from DCN. DCN is host-to-host, so to transfer buffers from TPU to TPU over DCN, we first need to transfer over PCIe to the host, then egress over the network, then ingress over the target host network, then over PCIe into HBM.</p> <h2 id="key-takeaways">Key Takeaways</h2> <ul> <li> <p>TPUs are simple and can in most cases be thought of as a matrix multiply unit connected to memory (super fast), other chips over ICI (rather fast), and the rest of the datacenter over DCN (somewhat fast).</p> </li> <li>Communication is limited by our various network bandwidths in order of speed: <ul> <li>HBM bandwidth: Between a TensorCore and its associated HBM.</li> <li>ICI bandwidth: Between a TPU chip and its nearest 4 or 6 neighbors.</li> <li>PCIe bandwidth: Between a CPU host and its associated tray(s) of chips.</li> <li>DCN bandwidth: Between multiple CPU hosts, typically hosts not connected by ICI.</li> </ul> </li> <li> <p><strong>Within a slice, TPUs are only connected to their nearest neighbors via ICI.</strong> This means communication over ICI between distant chips in a slice needs to hop over the intervening chips first.</p> </li> <li> <p><strong>Weight matrices need to be padded to at least size 128</strong> (256 on TPU v6) in both dimensions to fill up the MXU (in fact, smaller axes are padded to 128).</p> </li> <li> <p><strong>Lower precision matrix multiplication tends to be faster.</strong> TPUs can do int8 or int4 FLOPs roughly 2x/4x faster than bfloat16 FLOPs for generations that support it. VPU operations are still performed in fp32.</p> </li> <li> <p>To avoid bottlenecking the TPU compute unit, we need to <strong>make sure the amount of communication across each channel is proportional to its speed</strong>.</p> </li> <li><strong>Here are some specific numbers for our chips:</strong></li> </ul> <table> <thead> <tr> <th style="text-align: left">Model</th> <th style="text-align: center">Pod size</th> <th style="text-align: center">Host size</th> <th style="text-align: center">HBM capacity/chip</th> <th style="text-align: center">HBM BW/chip (bytes/s)</th> <th style="text-align: center">FLOPs/s/chip (bf16)</th> <th style="text-align: center">FLOPs/s/chip (int8)</th> </tr> </thead> <tbody> <tr> <td style="text-align: left"><span class="nowrap-header">TPU v3</span></td> <td style="text-align: center">32x32</td> <td style="text-align: center">4x2</td> <td style="text-align: center">32GB</td> <td style="text-align: center">9.0e11</td> <td style="text-align: center">1.4e14</td> <td style="text-align: center">1.4e14</td> </tr> <tr> <td style="text-align: left"><span class="nowrap-header">TPU v4p</span></td> <td style="text-align: center">16x16x16</td> <td style="text-align: center">2x2x1</td> <td style="text-align: center">32GB</td> <td style="text-align: center">1.2e12</td> <td style="text-align: center">2.75e14</td> <td style="text-align: center">2.75e14</td> </tr> <tr> <td style="text-align: left"><span class="nowrap-header">TPU v5p</span></td> <td style="text-align: center">16x20x28</td> <td style="text-align: center">2x2x1</td> <td style="text-align: center">96GB</td> <td style="text-align: center">2.8e12</td> <td style="text-align: center">4.59e14</td> <td style="text-align: center">9.18e14</td> </tr> <tr> <td style="text-align: left"><span class="nowrap-header">TPU v5e</span></td> <td style="text-align: center">16x16</td> <td style="text-align: center">4x2</td> <td style="text-align: center">16GB</td> <td style="text-align: center">8.1e11</td> <td style="text-align: center">1.97e14</td> <td style="text-align: center">3.94e14</td> </tr> <tr> <td style="text-align: left"><span class="nowrap-header">TPU v6e</span></td> <td style="text-align: center">16x16</td> <td style="text-align: center">4x2</td> <td style="text-align: center">32GB</td> <td style="text-align: center">1.6e12</td> <td style="text-align: center">9.20e14</td> <td style="text-align: center">1.84e15</td> </tr> </tbody> </table> <p>Host size refers to the topology of TPUs connected to a single host (e.g. TPU v5e has a single CPU host connected to 8 TPUs in a 4x2 topology). Here are interconnect figures:</p> <table> <thead> <tr> <th style="text-align: left">Model</th> <th style="text-align: center">ICI BW/link (one-way, bytes/s)</th> <th style="text-align: center">ICI BW/link (bidi, bytes/s)</th> </tr> </thead> <tbody> <tr> <td style="text-align: left"><strong>TPU v3</strong></td> <td style="text-align: center">1e11</td> <td style="text-align: center">2e11</td> </tr> <tr> <td style="text-align: left"><strong>TPU v4p</strong></td> <td style="text-align: center">4.5e10</td> <td style="text-align: center">9e10</td> </tr> <tr> <td style="text-align: left"><strong>TPU v5p</strong></td> <td style="text-align: center">9e10</td> <td style="text-align: center">1.8e11</td> </tr> <tr> <td style="text-align: left"><strong>TPU v5e</strong></td> <td style="text-align: center">4.5e10</td> <td style="text-align: center">9e10</td> </tr> <tr> <td style="text-align: left"><strong>TPU v6e</strong></td> <td style="text-align: center">9e10</td> <td style="text-align: center">1.8e11</td> </tr> </tbody> </table> <p>We include both one-way (unidirectional) bandwidth and bidi (bidirectional) bandwidth since unidirectional bandwidth is more true to the hardware but bidirectional bandwidth occurs more often in equations involving a full ring.<d-footnote>By bidi (bidirectional) bandwidth we mean the total bytes that can be sent along a single link in both directions, or equally, the total number of outgoing bytes from a single TPU along a particular axis, assuming we can use both links efficiently. This is true when we have a functioning ring, AKA when we have a wraparound connection on the particular axis. This occurs on inference chips when we have a full 16 axis, or on training chips (v*p) when we have an axis which is a multiple of 4. We prefer to use the bidirectional bandwidth because it appears frequently in calculations involving bidirectional comms.</d-footnote></p> <p>PCIe bandwidth is typically around <code class="language-plaintext highlighter-rouge">1.5e10</code> bytes / second per chip<d-footnote>Trillium (TPU v6e) has 32GB/s, about 2x higher than v5.</d-footnote>, while DCN bandwidth is typically around <code class="language-plaintext highlighter-rouge">2.5e10</code> bytes / second per host. We include both unidirectional and bidirectional bandwidth for completeness. Typically bidirectional bandwidth is the more useful number when we have access to a full wraparound ring, while one-way bandwidth is more true to the hardware.</p> <h2 id="worked-problems">Worked Problems</h2> <p>These numbers are a little dry, but they let you make basic roofline estimates for model performance. Let’s work a few problems to explain why this is useful. You’ll see more examples in Part 3.</p> <p><strong>Question 1 [bounding LLM latency]:</strong> Say you want to sample from a 200B parameter model in bf16 that’s split across 32 TPU v4p. How long would it take to load all the parameters from HBM into the systolic array? <em>Hint: use the numbers above.</em></p> <details><summary>Click here for the answer.</summary> <p><strong>Answer:</strong> We’re loading <code class="language-plaintext highlighter-rouge">sizeof(bf16) * 200e9 = 400e9</code> bytes on 32 chips, meaning 12.5e9 bytes / chip, each with an HBM bandwidth of 1.23e12. So the load takes around 10ms.</p> <p>That’s pretty cool, because <em>that’s a reasonable lower bound on the latency of sampling</em> from the model. Each sampling step needs to load all parameters from HBM, so it cannot take less than 10 ms. In practice, at small batch sizes, this is close to being achievable.</p> </details> <p><strong>Question 2 [TPU details]:</strong> Consider a full TPU v5e pod. How many total CPU hosts are there? How many TPU TensorCores? What is the total FLOPs/s for the whole pod? What is the total HBM? Do the same exercise for TPU v5p pod.</p> <details><summary>Click here for the answer.</summary> <p><strong>Answer:</strong> For TPU v5e, each pod is <code class="language-plaintext highlighter-rouge">16x16</code> and each host is a 4x2 slice, so we have <code class="language-plaintext highlighter-rouge">16*16 / 8 = 32</code> hosts. For TPU v5e, each TPU has only one core, so we have 256 TensorCores. The total FLOPs/s is <code class="language-plaintext highlighter-rouge">16*16*2e14 = 5.1e16</code> in bfloat16. Each chip has 16GB of HBM, so that’s <code class="language-plaintext highlighter-rouge">256 * 16 = 4TB</code> of memory.</p> <p>For a full TPU v5p pod, we have <code class="language-plaintext highlighter-rouge">16x20x28</code> chips and each host is 2x2x1, so we have <code class="language-plaintext highlighter-rouge">16*20*28 / 2*2 = 2,240</code> hosts. For TPU v5p, each TPU has two TensorCores, so we have <code class="language-plaintext highlighter-rouge">8960 * 2 = 17,920</code> cores. The total FLOPs/s is <code class="language-plaintext highlighter-rouge">8960 * 4.5e14 = 4e18</code> in bfloat16. Each chip has 96GB of HBM, so that’s <code class="language-plaintext highlighter-rouge">8960 * 96 = 860TB</code> of memory.</p> </details> <p><strong>Question 3 [PCIe operational intensity]:</strong> Imagine we’re forced to store a big weight matrix $A$ of type $\text{bfloat16}[D, F]$, and a batch of activations $x$ of type $\text{bfloat16}[B, D]$ in host DRAM and want to do a matrix multiplication on them. This is running on a single host, and we’re using a single TPU v6e chip attached to it. You can assume $B \ll D$, and $F = 4D$ (we’ll see in future chapters why these are reasonable assumptions). What is the smallest batch size $B$ we need to remain FLOPs bound over PCIe? Assume PCIe bandwidth of 1.5e10 bytes / second.</p> <details><summary>Click here for the answer.</summary> <p><strong>Answer:</strong> We have to perform $2BDF$ floating point operations, and each chip can perform <code class="language-plaintext highlighter-rouge">9.2e14</code> floating point operations per second. This then requires $2BDF / 9.2e14$ seconds to perform. We have to load $2DF + 2BD$ bytes from DRAM, and write $2BF$ bytes back to it. We are bottlenecked by PCIe transfer speeds, so we need $2 \cdot (BD + DF + BF) / 1.5e10$ seconds to transfer data to and from the TPU. Since we want computation to take longer than weight loading, assuming we can overlap all weight loading with computation, we want $2BDF / 9.2e14 &gt; 2 \cdot (BD + DF + BF) / 1.5e10$. We can simplify this using our assumptions that $B \ll D$, and $F = 4D$, to get</p> \[\frac{8BD^2}{9.2e14} &gt; \frac{8D^2}{1.5e10}\] <p>or</p> \[B &gt; \frac{9.2e14}{1.5e10} \simeq 61,000\] </details> <p><strong>Question 4 [general matmul latency]:</strong> Let’s say we want to multiply a weight matrix int8[16384, 4096] by an activation matrix of size int8[B, 4096] where B is some unknown batch size. Let’s say we’re on 1 TPUv5e to start.</p> <ol> <li>How long will this multiplication take as a function of B? <em>Hint: it may help to calculate how long it will take to load the arrays from HBM and how long the multiplication will actually take. Which is bottlenecking you?</em> </li> <li>What if we wanted to run this operation out of VMEM? How long would it take as a function of B?</li> </ol> <details><summary>Click here for the answer.</summary> <p><strong>Answer:</strong> (1) The number of floating point operations we need to perform is $2 \cdot 4096 \cdot 16384 \cdot B = 1.3e8 \cdot B$. So $T_{\text{math}} = (1.3e8 \cdot B) / 3.94e14$ seconds. We need to load $16384 \cdot 4096 + 4096 \cdot B$ bytes from HBM to VMEM, and write back $16384 \cdot B$ bytes from VMEM to HBM. This means $T_{\text{comms}} = (6.7e7 + 2e4\cdot B) / 8.1e11$ seconds. Assuming as much overlap of communication and computation as possible, the whole multiplication will take approximately</p> \[\max\{T_{\text{math}}, T_{\text{comms}}\} = \max\left\{\frac{6.7e7 + 2e4\cdot B}{8.1e11}, \frac{1.3e8 \cdot B}{3.94e14}\right\}\] <p>We’ll be FLOPs-bound when $\frac{6.7e7 + 2e4\cdot B}{8.1e11} &lt; \frac{1.3e8 \cdot B}{3.94e14}$, or equivalently, $B &gt; 271$. This is slightly larger than the 240 number we derive below because we factor in the full impact of \(D\) and \(F\).</p> <p>(2) If instead we are loading from VMEM, let’s consider VMEM bandwidth to the MXU as 22 times the HBM $\leftrightarrow$ VMEM bandwidth. This turns our data loading denominator from 8.1e11 to 1.78e13, and we get $B &gt; 11$. Note that in practice, we cannot dedicate all of our VMEM bandwidth to loading $W$, so in practice it will be closer to 20.</p> </details> <p><strong>Question 5 [ICI bandwidth]:</strong> Let’s say we have a TPU v5e <code class="language-plaintext highlighter-rouge">4x4</code> slice. Let’s say we want to send an array of type <code class="language-plaintext highlighter-rouge">bfloat16[8, 128, 8192]</code> from <code class="language-plaintext highlighter-rouge">TPU{0,0}</code> to <code class="language-plaintext highlighter-rouge">TPU{3, 3}</code>. Let’s say the per-hop latency for TPU v5e is $1\mu s$.</p> <ol> <li>How soon will the first byte arrive at its destination?</li> <li>How long will the total transfer take?</li> </ol> <details><summary>Click here for the answer.</summary> <p><strong>Answer:</strong> In a TPUv5e we have 2D connectivity. Because we have only a <code class="language-plaintext highlighter-rouge">4x4</code> slice (with no axes of size 16), we have no wraparound connections. Thus there are two ports from which our target chip can receive data, and likewise two ports from which our source chip can send data. The amount of data we have to transfer is <code class="language-plaintext highlighter-rouge">2 * 8 * 128 * 8192 = 1.7e7</code> bytes. We can transfer from both ports simultaneously (i.e. send half the array right and half down), so we get <code class="language-plaintext highlighter-rouge">2 * 4.5e10 = 9e10</code> bytes transferred per second, which means it’ll take about <code class="language-plaintext highlighter-rouge">1.7e7 / 9e10 = 188us</code> to transfer the whole array through (assuming we’re bandwidth bound). In a <code class="language-plaintext highlighter-rouge">4x4</code> slice, we have six hops between chips $(0, 0)$ and $(3, 3)$, since there are no wraparound links for axes with fewer than 16 chips. Since the latency of each hop is about $1\mu s$, the first byte will arrive in about<code class="language-plaintext highlighter-rouge">6us</code> and the total transfer will take <code class="language-plaintext highlighter-rouge">188us</code>.</p> </details> <p><strong>Question 6 [pulling it all together, hard]:</strong> Imagine you have a big matrix <strong>A</strong>: <code class="language-plaintext highlighter-rouge">int8[128 * 1024, 128 * 1024]</code> sharded evenly across a TPU v5e 4x4 slice but offloaded to host DRAM on each chip. Let’s say you want to copy the entire array to TPU{0, 0} and multiply it by a vector <code class="language-plaintext highlighter-rouge">bf16[8, 128 * 1024]</code>. How long will this take? <em>Hint: use the numbers above.</em></p> <details><summary>Click here for the answer.</summary> <p><strong>Answer:</strong> Let’s start by outlining the operations we have to perform. Our array is about 16GB. From the table above, a TPU v5e host has a 4x2 topology, so a 4x4 has 2 hosts, Thus, since our array is evenly sharded, each host effectively contains a chunk of 1/2 of the array, or 8GB. We need to copy these chunks all to TPU{0,0}, which gives us two options:</p> <ol> <li>We can copy over DCN and then load the entire unsharded array over PCIe into HBM.</li> <li>We can load our sharded arrays onto their corresponding TPUs, then perform a gather over ICI, then perform the matmul on TPU{0,0}.</li> </ol> <p>It should be clear that option (2) is better. DCN is slow compared to ICI and we’d much prefer to load a big array over many PCIe links rather than just a few (the 8 on host 0). Here’s a diagram of part of the system. As described above, note that TPUs are connected to their neighbors by ICI (even across hosts), all TPUs are connected to their host CPU (via PCIe), and hosts are connected by DCN.</p> <figure> <picture> <source class="responsive-img-srcset" srcset="/scaling-book/assets/img/challenge-problem-480.webp 480w,/scaling-book/assets/img/challenge-problem-800.webp 800w,/scaling-book/assets/img/challenge-problem-1400.webp 1400w," type="image/webp" sizes="95vw"></source> <img src="/scaling-book/assets/img/challenge-problem.png" class="img-fluid img-small" width="100%" height="auto" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> <figcaption class="caption">Each chip actually has its own PCIe link to its host, though for clarity only one is shown here.</figcaption> </figure> <p>Now let’s work through how long each piece will take:</p> <ol> <li> <p><strong>PCIe load</strong>: we’re loading chunks of 16GB / 2 = 8GB over 16 PCIe links, each of which has <code class="language-plaintext highlighter-rouge">1.5e10</code> bytes/second bandwidth. Thus this will take about 33ms.</p> </li> <li> <p><strong>ICI copy:</strong> each TPU now has 16GB / 16 = 1GB of our array. Our ICI bandwidth is 9e10 bytes/second per link <em>bidirectional</em>, and you’ll notice from the above diagram that only 2 of the 4 ICI links on the TPU v5e are in use in this topology for TPU{0,0}. Since TPU{0,0} needs to receive a total of 15GB along 2 axes at <code class="language-plaintext highlighter-rouge">4.5e10</code> bytes/s/link, we can lower bound the time by <code class="language-plaintext highlighter-rouge">15e9 / (4.5e10 * 2) = 167ms</code>. In practice this probably isn’t achievable because the load is very uneven, but it’s probably within a factor of 2. As you’ll see in Section 2, performing a full AllGather would also take roughly <code class="language-plaintext highlighter-rouge">16e9 / (4.5e10 * 2)</code>, so this is close to optimal.</p> </li> <li> <p><strong>HBM $\rightarrow$ MXU load:</strong> to perform our final matmul, we need to load these 16e9 bytes plus the bf16[8, 128 * 1024] array (another 2MB, so negligible) over HBM bandwidth into the MXU, which will take <code class="language-plaintext highlighter-rouge">16e9 / 8.1e11 = 19ms</code>.</p> </li> <li> <p><strong>FLOPs:</strong> we’re performing a total of \(2 \cdot 8 \cdot 128 \cdot 1024 \cdot 128 \cdot 1024 = 2.7e11\) FLOPs, and since we can perform <code class="language-plaintext highlighter-rouge">1.97e14</code> bf16 FLOPs/s, we get 1.3ms.</p> </li> </ol> <p>An upper bound for the total time is the sum of all of these times, but since the TPU can typically overlap these operations, we can think of this as a pipelining problem that’s bottlenecked by the slowest piece. Assuming that’s true, then the answer is about 150-200ms.</p> </details> <h3 class="next-section">That’s it for Part 2! For Part 3, covering partitioning and cross-TPU communication, <a href="../sharding">click here</a>.</h3> <h2 id="appendix">Appendix</h2> <h3 id="appendix-a-all-about-gpus">Appendix A: All about GPUs</h3> <p>Since the Volta generation (V100), TPUs and GPUs have started to looked a lot alike: <em>they both aim to do matrix multiplication very fast</em>. They both act as an accelerator attached to a CPU and many components are roughly analogous (don’t worry if you don’t know all the terminology, we’ll introduce them all later):</p> <table> <thead> <tr> <th style="text-align: center">TPU</th> <th style="text-align: center">GPU</th> </tr> </thead> <tbody> <tr> <td style="text-align: center">Tensor Core</td> <td style="text-align: center">SM (“Streaming Multiprocessor”)</td> </tr> <tr> <td style="text-align: center">HBM</td> <td style="text-align: center">DRAM</td> </tr> <tr> <td style="text-align: center">VMEM</td> <td style="text-align: center">SMEM (often used as an L1 cache)</td> </tr> <tr> <td style="text-align: center">VPU</td> <td style="text-align: center">Warp scheduler (a set of SIMD CUDA cores)</td> </tr> <tr> <td style="text-align: center">MXU</td> <td style="text-align: center">Tensor Core</td> </tr> <tr> <td style="text-align: center">ICI</td> <td style="text-align: center">NVLink/NVSwitch</td> </tr> </tbody> </table> <p>The core unit of a GPU is an SM, or “streaming multiprocessor”, which is roughly analogous to the whole TPU Tensor Core described above. Compared to TPUs, though, GPUs have <em>many</em> more of them (an H100 has about 144). Each SM has its own matrix multiplication unit, confusingly called a Tensor Core, which acts like the TPU MXU, and a set of 4 narrow SIMD units called Warp schedulers that act like the TPU VPUs (with 32 lanes instead of 1024). More independent SMs makes computation more flexible (since each can do totally independent work) but also makes the hardware more expensive and complex to reason about.</p> <figure> <picture> <source class="responsive-img-srcset" srcset="/scaling-book/assets/img/b100-sm-diagram-480.webp 480w,/scaling-book/assets/img/b100-sm-diagram-800.webp 800w,/scaling-book/assets/img/b100-sm-diagram-1400.webp 1400w," type="image/webp" sizes="95vw"></source> <img src="/scaling-book/assets/img/b100-sm-diagram.png" class="img-small" width="100%" height="auto" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> <figcaption class="caption"><b>Figure:</b> the basic components of a Blackwell (B100) SM. The diagram shows 4 SIMD compute units (which we call warp schedulers), each with a Tensor Core for matrix multiplication. This also shows per-warp scheduler registers, an SM-level L1 cache, and TMEM or tensor memory which is a new addition in Blackwell.</figcaption> </figure> <p>Each SM also has an O(256kB) L1 cache (also called SMEM) used to speed data access and for register spilling. A section of the memory used for the L1 cache can also be declared as shared memory allowing access from any thread in the thread-block, and is used for user-defined caches, parallel reductions and synchronization, etc. (similar to VMEM on a TPU).</p> <p>GPUs also have an additional L2 cache that is shared by all SMs. Unlike VMEM, this is hardware managed and optimizing cache hits is often important for performance.</p> <p><strong>Networking:</strong></p> <ul> <li>Primary difference is that NVIDIA GPUs are typically in ‘cliques’ of 8-256 GPUs via switches (NVLink $\rightarrow$ NVSwitch), which allow for point-to-point communication between any GPU within that ‘clique’, but that means communication between more than 256 is significantly slower - this means training on more than 256 typically requires pipeline parallelism to scale, which is more complex (by contrast, PaLM was trained on two cliques of 3072 TPU chips each).</li> <li>For common neural net operations such as AllReduce, all-to-all connections do not hold an advantage (as the same communication patterns must occur regardless), but it does allow for storing MoE models across more GPUs and transmitting the experts around more efficiently.</li> <li>Each GPU requires a switch that costs similar to the GPU itself, making on chip interconnect like ICI cheaper.</li> <li><a href="https://docs.nvidia.com/deeplearning/performance/dl-performance-gpu-background/index.html#gpu-arch" rel="external nofollow noopener" target="_blank">NVIDIA deep learning performance</a></li> <li><a href="https://www.nvidia.com/en-au/data-center/nvlink/" rel="external nofollow noopener" target="_blank">NVSwitch</a></li> <li>Very different Tensor Parallelism / Pipeline Parallelism transition point!</li> </ul> <h3 id="appendix-b-how-does-a-systolic-array-work">Appendix B: How does a systolic array work?</h3> <p>At the core of the TPU MXU is a <code class="language-plaintext highlighter-rouge">128x128</code> systolic array (<code class="language-plaintext highlighter-rouge">256x256</code> on TPU v6e). When fully saturated the systolic array can perform one <code class="language-plaintext highlighter-rouge">bfloat16[8,128] @ bf16[128x128] -&gt; f32[8,128]</code><d-footnote>If you are not familiar with this notation, it means: multiplying a `8x128` matrix with bfloat16 elements by a `128x128` matrix with bfloat16 elements and storing the results in a `8x128` matrix with float32 elements.</d-footnote> multiplication per 8 clock cycles.</p> <ul> <li>At its core, the systolic array is a 2D <code class="language-plaintext highlighter-rouge">128x128</code> (<code class="language-plaintext highlighter-rouge">=16,384</code>) grid of ALUs each capable of performing a multiply and add operation.</li> <li>Weights (<strong>W</strong>, the <code class="language-plaintext highlighter-rouge">128x128</code> input) are passed down from above (called the RHS) while inputs (<strong>X</strong>, the <code class="language-plaintext highlighter-rouge">8x128</code> input) are passed in from the left (called the LHS).</li> </ul> <p>Here is a simplified animation of multiplying a set of weights (blue) with a set of activations (green). You’ll notice that the weights (RHS) are partially loaded first, diagonally, and then the activations are fed in, also diagonally. In each frame below, we multiply all the overlapped green and blue units, sum the result with any residual passed in from above, and then pass the result in turn down one unit.</p> <figure> <picture> <source class="responsive-img-srcset" srcset="/scaling-book/assets/img/systolic-array-480.webp 480w,/scaling-book/assets/img/systolic-array-800.webp 800w,/scaling-book/assets/img/systolic-array-1400.webp 1400w," type="image/webp" sizes="95vw"></source> <img src="/scaling-book/assets/img/systolic-array.gif" width="100%" height="auto" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> <p>Here’s a more general version of this animation showing the output being streamed out of computation:</p> <figure> <picture> <source class="responsive-img-srcset" srcset="/scaling-book/assets/img/systolic-array2-480.webp 480w,/scaling-book/assets/img/systolic-array2-800.webp 800w,/scaling-book/assets/img/systolic-array2-1400.webp 1400w," type="image/webp" sizes="95vw"></source> <img src="/scaling-book/assets/img/systolic-array2.gif" class="img-small" width="100%" height="auto" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> <p>Here’s a diagram showing how this can be pipelined across multiple RHS and LHS arrays:</p> <figure> <picture> <source class="responsive-img-srcset" srcset="/scaling-book/assets/img/systolic-array-pipelining-480.webp 480w,/scaling-book/assets/img/systolic-array-pipelining-800.webp 800w,/scaling-book/assets/img/systolic-array-pipelining-1400.webp 1400w," type="image/webp" sizes="95vw"></source> <img src="/scaling-book/assets/img/systolic-array-pipelining.png" class="img-fluid" width="100%" height="auto" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> <p>There is an initial pipeline bubble as the weights (RHS) and activations (LHS) are loaded. After that initial bubble, new inputs and weights can be loaded in without an additional bubble.</p> <p>Here’s a bad animation of a bf16[2, 3] x bf16[3, 3] matrix multiplication, which you could imagine as a matmul of a 2x3 weight matrix with an input activation of batch 1 and size 3. This is rotated compared to the previous slides and inputs flow out to the right instead of down, but you can roughly see the structure.</p> <figure> <picture> <source class="responsive-img-srcset" srcset="/scaling-book/assets/img/systolic-array-bad-480.webp 480w,/scaling-book/assets/img/systolic-array-bad-800.webp 800w,/scaling-book/assets/img/systolic-array-bad-1400.webp 1400w," type="image/webp" sizes="95vw"></source> <img src="/scaling-book/assets/img/systolic-array-bad.gif" class="img-small" width="100%" height="auto" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> <p>We can efficiently pipeline this to multiply large matrices without too large a pipeline bubble. With that said, it’s important that our matrices have shapes larger than the side dimension of the MXU, which is generally 128x128. Some TPUs (since TPU v3) have multiple MXUs, either 2 for TPU v3 and 4 for TPU v4/5, so we need to ensure tiling dimensions are larger than 128 * number of MXUs. <a href="https://www.youtube.com/watch?v=sJltBQ4MOHA" rel="external nofollow noopener" target="_blank">Here’s</a> a good animation for this.</p> <p>Trillium (TPU v6e) has a <code class="language-plaintext highlighter-rouge">256x256</code> systolic array, which means it can perform 4x more FLOPs / cycle. This also means the dimensions of your tensors needs to be twice as large to utilize the MXU fully.</p> <p><a href="https://fleetwood.dev/posts/domain-specific-architectures#google-tpu" rel="external nofollow noopener" target="_blank">This blog post</a> has another excellent animation of a systolic array multiplication for a fixed weight matrix.</p> <h3 id="appendix-c-more-on-tpu-internals">Appendix C: More on TPU internals</h3> <h3 id="scalar-core">Scalar Core</h3> <p>The scalar core is the control unit of the TPU. It fetches and dispatches all instructions and executes transfers from HBM into VMEM, and can be programmed to do scalar metadata work. Because the scalar core is single-threaded, one side-effect of this is that each core of the TPU is only capable of creating one DMA request per cycle.</p> <p>To put this in context, a single scalar core controls a VPU consisting of 2048 ALUs, 4 MXUs, 2 XLUs, and multiple DMA engines. The highly skewed nature of control per unit compute is a source of hardware efficiency, but also limits the ability to do data dependent vectorization in any interesting way.</p> <h3 id="vpu">VPU</h3> <p>The TPU vector core consists of a two dimensional SIMD vector machine (the <strong>VPU</strong>) that performs vector operations like vadd (vector addition) or vmax (elementwise max) and a set of vector registers called <strong>VREGs</strong> that hold data for the VPU and MXU. Each TPU core for v5p has 64 32-bit VREGs (32 in v4), giving us a total of about <code class="language-plaintext highlighter-rouge">64 * 8 * 128 * 4 = 256kB</code> of VREG memory.</p> <p>The VPU is effectively a 2D vector arithmetic unit of shape <code class="language-plaintext highlighter-rouge">(8, 128)</code> where the 128 dimension is referred to as lane axis and the dimension of 8 is referred to as the sublane axis. Each (lane, sublane) pair on v5 contains 4 standard floating-point and integer ALUs. From a software point-of-view, this creates the appearance of a 8x128 vector unit with a total of 4048 floating point adders in v5.</p> <p>The VPU executes most arithmetic instructions in one cycle in each of its ALUs (like vadd or vector add) with a latency of 2 cycles, so e.g. in v5 you can add 4 pairs of f32 values together from VREGs in each cycle. A typical VPU instruction might look like <code class="language-plaintext highlighter-rouge">{v2 = vadd.8x128.f32 v0, v1}</code> where v0 and v1 are input VREGs and v2 is an output VREG.</p> <p>All lanes and sublanes execute the same program every cycle in a pure SIMD manner, but each ALU can perform a different operation. So we can e.g. process 1 vadd and 1 vsub in a single cycle, each of which operates on two full VREGs and writes the output to a third.</p> <p>Reductions within a lane (over the size-8 sublane dimension) are cheap and very efficient (3 permutes and 3 adds). Cross-lane reductions are harder and involve the XLU or “cross lane unit”, which is slow and fairly expensive.</p> </d-article> <d-appendix> <d-footnote-list></d-footnote-list> <d-citation-list></d-citation-list> <div class="base-grid appendix-entry"> <h3 style="grid-column: 0;">Miscellaneous</h3> <p class="author-footnote" style="grid-column: text;"><sup>*</sup>Work done at Google DeepMind, now at MatX.</p> </div> <div class="base-grid appendix-entry"> <h3 style="grid-column: 0;">Citation</h3> <p class="author-footnote">For attribution in academic contexts, please cite this work as:</p> <div class="author-footnote"> <div class="language-bibtex highlighter-rouge"><div class="highlight"><pre class="highlight"><code>    <span class="c">Austin et al., "How to Scale Your Model", Google DeepMind, online, 2025.</span>
</code></pre></div></div> </div> <p class="author-footnote">or as a BibTeX entry:</p> <div class="author-footnote"> <div class="language-bibtex highlighter-rouge"><div class="highlight"><pre class="highlight"><code>    <span class="nc">@article</span><span class="p">{</span><span class="nl">scaling-book</span><span class="p">,</span>
      <span class="na">title</span> <span class="p">=</span> <span class="s">{How to Scale Your Model}</span><span class="p">,</span>
      <span class="na">author</span> <span class="p">=</span> <span class="s">{Austin, Jacob and Douglas, Sholto and Frostig, Roy and Levskaya, Anselm and Chen, Charlie and Vikram, Sharad
      and Lebron, Federico and Choy, Peter and Ramasesh, Vinay and Webson, Albert and Pope, Reiner}</span><span class="p">,</span>
      <span class="na">publisher</span> <span class="p">=</span> <span class="s">{Google DeepMind}</span><span class="p">,</span>
      <span class="na">howpublished</span> <span class="p">=</span> <span class="s">{Online}</span><span class="p">,</span>
      <span class="na">note</span> <span class="p">=</span> <span class="s">{Retrieved from https://jax-ml.github.io/scaling-book/}</span><span class="p">,</span>
      <span class="na">year</span> <span class="p">=</span> <span class="s">{2025}</span>
    <span class="p">}</span>
</code></pre></div></div> </div> </div> </d-appendix> <d-bibliography src="/scaling-book/assets/bibliography/main.bib"></d-bibliography> <div id="giscus_thread" style="max-width: 930px; margin: 0 auto;"> <script>
      let giscusTheme = determineComputedTheme();
      let giscusAttributes = {
        src: 'https://giscus.app/client.js',
        'data-repo': 'jax-ml/scaling-book',
        'data-repo-id': '',
        'data-category': 'General',
        'data-category-id': '',
        'data-mapping': 'title',
        'data-strict': '0',
        'data-reactions-enabled': '1',
        'data-emit-metadata': '0',
        'data-input-position': 'bottom',
        'data-theme': giscusTheme,
        'data-loading': '1',
        'data-lang': 'en',
        crossorigin: 'anonymous',
        async: '',
      };

      let giscusScript = document.createElement('script');
      Object.entries(giscusAttributes).forEach(([key, value]) => giscusScript.setAttribute(key, value));
      document.getElementById('giscus_thread').appendChild(giscusScript);
    </script> <noscript>Please enable JavaScript to view the <a href="http://giscus.app/?ref_noscript" rel="external nofollow noopener" target="_blank">comments powered by giscus.</a> </noscript> </div> </div> <footer class="sticky-bottom mt-5" role="contentinfo"> <div class="container"> © Copyright 2025 . Powered by <a href="https://jekyllrb.com/" target="_blank" rel="external nofollow noopener">Jekyll</a> with <a href="https://github.com/alshedivat/al-folio" rel="external nofollow noopener" target="_blank">al-folio</a> theme. Hosted by <a href="https://pages.github.com/" target="_blank" rel="external nofollow noopener">GitHub Pages</a>. </div> </footer> <script src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js" integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin="anonymous"></script> <script src="/scaling-book/assets/js/bootstrap.bundle.min.js"></script> <script src="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/js/mdb.min.js" integrity="sha256-NdbiivsvWt7VYCt6hYNT3h/th9vSTL4EDWeGs5SN3DA=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/medium-zoom@1.1.0/dist/medium-zoom.min.js" integrity="sha256-ZgMyDAIYDYGxbcpJcfUnYwNevG/xi9OHKaR/8GK+jWc=" crossorigin="anonymous"></script> <script defer src="/scaling-book/assets/js/zoom.js?85ddb88934d28b74e78031fd54cf8308"></script> <script src="/scaling-book/assets/js/no_defer.js?2781658a0a2b13ed609542042a859126"></script> <script defer src="/scaling-book/assets/js/common.js?e0514a05c5c95ac1a93a8dfd5249b92e"></script> <script defer src="/scaling-book/assets/js/copy_code.js?12775fdf7f95e901d7119054556e495f" type="text/javascript"></script> <script defer src="/scaling-book/assets/js/jupyter_new_tab.js?d9f17b6adc2311cbabd747f4538bb15f"></script> <script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.2.2/es5/tex-mml-chtml.js" integrity="sha256-MASABpB4tYktI2Oitl4t+78w/lyA+D7b/s9GEP0JOGI=" crossorigin="anonymous"></script> <script src="/scaling-book/assets/js/mathjax-setup.js?70d799092f862ad98c7876aa47712e20"></script> <script defer src="https://cdnjs.cloudflare.com/polyfill/v3/polyfill.min.js?features=es6" crossorigin="anonymous"></script> <script defer src="/scaling-book/assets/js/progress-bar.js?2f30e0e6801ea8f5036fa66e1ab0a71a" type="text/javascript"></script> <script src="/scaling-book/assets/js/vanilla-back-to-top.min.js?f40d453793ff4f64e238e420181a1d17"></script> <script>
    addBackToTop();
  </script> </body> </html>